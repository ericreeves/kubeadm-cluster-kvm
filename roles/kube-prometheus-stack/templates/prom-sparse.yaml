# full values: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml

{% if k8s_implementation == "kubeadm" %}

kubeEtcd:
  enabled: true
  service:
    enabled: true
    port: 2381
    targetPort: 2381

{% elif k8s_implementation == "k3s" %}

# k3s uses sqllite, so we cannot monitor this component
defaultRules:
  rules:
    etcd: false

kubeEtcd:
  enabled: false

# matched to service port 'prom-stack-kube-prometheus-kube-controller-manager' -n kube-system
kubeControllerManager:
  enabled: true
  endpoints: ['{{master_ip_internal}}']
  service:
    enabled: true
    port: 10252
    targetPort: 10252
  serviceMonitor:
    enabled: true
    https: false

# matched to service port 'prom-stack-kube-prometheus-kube-scheduler' -n kube-system
kubeScheduler:
  enabled: true
  endpoints: ['{{master_ip_internal}}']
  service:
    enabled: true
    port: 10251
    targetPort: 10251
  serviceMonitor:
    enabled: true
    https: false

# matched to service port 'prom-stack-kube-prometheus-kube-proxy' -n kube-system
kubeProxy:
  enabled: true
  endpoints: ['{{master_ip_internal}}']
  service:
    enabled: true
    port: 10249
    targetPort: 10249

{% endif %}

alertmanager:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
{% if prometheus_ingress_at == "subdomain" %}
    hosts: ['alertmanager.{{ cert_domains_list | first }}']
    paths: ['/']
    tls:
    - secretName: {{secret_name}}
      hosts:
      - alertmanager.{{ cert_domains_list | first }}
{% elif prometheus_ingress_at == "context" %}
      nginx.ingress.kubernetes.io/rewrite-target: /$2
    hosts: ['{{ cert_domains_list | first }}']
    paths: ['/alertmanager(/|$)(.*)']
    tls:
    - secretName: {{secret_name}}
      hosts:
      - {{ cert_domains_list | first }}
{% endif %}

  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: {{storage_class}}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
{% if prometheus_ingress_at == "subdomain" %}
    externalUrl: https://alertmanager.{{ cert_domains_list | first }}/
{% elif prometheus_ingress_at == "context" %}
    externalUrl: https://{{ cert_domains_list | first }}/alertmanager
{% endif %}
    routePrefix: /

  config:
    global:
      resolve_timeout: 5m
      # global smtp settings
      smtp_from: amgr@{{k8s_implementation}}
      smtp_smarthost: {{smtp_host_port}}
      smtp_require_tls: false

    route:
      group_by: ['alertname'] # not default job
      group_wait: 2s # not default 30
      group_interval: 30s # not default 5m
      repeat_interval: 4h # not default 12h
      receiver: email_platform
      routes:
      - receiver: 'null'
        matchers:
          - alertname =~ "InfoInhibitor|Watchdog"
      - receiver: email_platform
        continue: true
    receivers:
    # https://prometheus.io/docs/alerting/latest/configuration/#email_config
    - name: email_platform
      email_configs:
      - to: platform@{k8s_implementation}}
        send_resolved: true
        headers:
          # used jinja string to avoid needing to escape curly brackets (which should not be ansible evaluated)
          subject: "{{ '{{ .Status | toUpper }} {{ .CommonLabels.mycluster }}:{{ .CommonLabels.namespace }}:{{ .CommonLabels.alertname }} {{ .CommonAnnotations.summary }}' }}"
    - name: 'null'
    templates:
    - '/etc/alertmanager/config/*.tmpl'

grafana:
{% if prometheus_ingress_at == "context" %}
  env:
    GF_SERVER_ROOT_URL: https://{{ cert_domains_list | first }}/grafana
    GF_SERVER_SERVE_FROM_SUB_PATH: 'true'
{% endif %}
  # username is 'admin'
  adminPassword: prom-operator
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
{% if prometheus_ingress_at == "subdomain" %}
    hosts: ['grafana.{{ cert_domains_list | first }}']
    path: "/"
    tls:
    - secretName: {{secret_name}}
      hosts:
      - grafana.{{ cert_domains_list | first }}
{% elif prometheus_ingress_at == "context" %}
      nginx.ingress.kubernetes.io/rewrite-target: /$2
    hosts: ['{{ cert_domains_list | first }}']
    path: "/grafana(/|$)(.*)"
    tls:
    - secretName: {{secret_name}}
      hosts:
      - {{ cert_domains_list | first }}
{% endif %}

prometheus:
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
{% if prometheus_ingress_at == "subdomain" %}
    hosts: ['prometheus.{{ cert_domains_list | first }}']
    paths: ['/']
    tls:
    - secretName: {{secret_name}}
      hosts:
      - prometheus.{{ cert_domains_list | first }}
{% elif prometheus_ingress_at == "context" %}
    hosts: ['{{ cert_domains_list | first }}']
    paths: ['/prometheus'] # does not need regex capture like others, leave off trailing forward slash
    tls:
    - secretName: {{secret_name}}
      hosts:
      - {{ cert_domains_list | first }}
{% endif %}

  prometheusSpec:
{% if prometheus_ingress_at == "subdomain" %}
    externalUrl: "https://prometheus.{{ cert_domains_list | first }}/"
    routePrefix: /
{% elif prometheus_ingress_at == "context" %}
    externalUrl: "https://{{ cert_domains_list | first }}/prometheus"

    # link from alertmanager to source does not work when using same domain
    # this is problem with the elm file
    # https://github.com/prometheus/alertmanager/issues/1881
    # https://github.com/prometheus/alertmanager/pull/2470/files
    routePrefix: /prometheus
{% endif %}

    # do not require new PrometheusRule to have all the helm labels in order to match
    ruleSelectorNilUsesHelmValues: false
   
    # additional scrape job
    additionalScrapeConfigs:

      - job_name: kubernetes-service-endpoints
        kubernetes_sd_configs:
        - {role: service}
        relabel_configs:
        - action: keep
          regex: true
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        - action: drop
          regex: (kube-system|prom)
          source_labels: [__meta_kubernetes_namespace]
        - action: keep
          regex: .*metrics
          source_labels: [__meta_kubernetes_service_port_name]
        - action: replace
          regex: (https?)
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          target_label: __scheme__
        - action: replace
          regex: (.+)
          source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          target_label: __metrics_path__
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          target_label: __address__
        - {action: labelmap, regex: __meta_kubernetes_service_label_(.+)}
        - action: replace
          source_labels: [__meta_kubernetes_namespace]
          target_label: kubernetes_namespace
        - action: replace
          source_labels: [__meta_kubernetes_service_name]
          target_label: kubernetes_name

    # external labels will be common for all alerts and available for templating in AlertManager
    externalLabels: {'cluster': 'kubeadm', 'env': 'dev', 'jumpbox': 'localhost.local'}

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: {{storage_class}}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi
